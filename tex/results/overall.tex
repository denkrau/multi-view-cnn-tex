\section{Overall Performance}
\label{sec:results-overall}
For evaluating the overall performances of the network architecture, the choice of hyperparameters is explained.
In \figref{fig:optimal-learning-rate} the increase of the learning rate against the related loss is shown for finding the range of optimal learning rates.
Additionally, the gradient of loss with respect to the learning rate is outlined.
This is performed once on the 0-3 network.
At a learning rate of approximately $\gamma = 10^{-4}$ the loss starts to decrease slightly.
At approximately $\gamma=0.004$ its decrease gets strikingly faster until at approximately $\gamma=0.01$ it starts to increase drastically.
Although for this network, in particular, a learning rate of $\gamma=10^{-2}$ seems to be suited well, a general initial learning rate of $\gamma = 10^{-3}$ is chosen.
On one hand, this is the most basic network, hence, it is supposed, that for more complicated ones, a smaller learning rate is better suited due to the more complex cost function.
Furthermore, interpreting those graphs is time-consuming and for more complicated networks not that easy anymore, because the loss changes more rapidly.
On the other hand, a learning rate of $\gamma=10^{-2}$ is close to the increase.
Hence, if the learning rate is shifted, the parameters of the network would be changed tremendously.
It was actually verified, that a learning rate of $\gamma=10^{-3}$ is a satisfiable choice for more complex networks because it lies close to the upper bound of the optimal learning rates range.
As a default value for all networks, it works as well, though.
\begin{figure}
	\setlength\figureheight{.3\textwidth}
	\setlength\figurewidth{.45\textwidth}
	\centering
	\begin{subfigure}{.5\textwidth}
		\centering
		\input{images/mn-sl-0-3-20/optimal_learning_rate_loss.tikz}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\centering
		\input{images/mn-sl-0-3-20/optimal_learning_rate_dloss.tikz}
	\end{subfigure}
	\caption[Optimal learning rate for the 0-3 network]{Optimal learning rate for the 0-3 network. Learning rate is initialized with 0.00001 and multiplied by 1.02 every iteration.}
	\label{fig:optimal-learning-rate}
\end{figure}

Furthermore, the decreased filter size in the first convolutional layer from $11 \times 11$ to $7 \times 7$ compared to the original AlexNet configuration is evaluated.
The reason for this decrease is the large filter size of $11 \times 11$ compared to more recent networks.
The filter sizes of the remaining layers match or are only slightly larger compared to those networks, hence, they are not decreased and evaluated.
In \figref{fig:first-conv-filter} the losses of the training process of both filter configurations of the first convolutional layer are shown for a single run.
\begin{figure}
	\setlength\figureheight{.4\textwidth}
	\setlength\figurewidth{.9\textwidth}
	\centering
	\input{images/first_conv_filter_size.tikz}
	\caption{Comparison of filter sizes of first convolutional layer based on loss}
	\label{fig:first-conv-filter}
\end{figure}
It can be seen, that with the smaller filter the loss decreases over time, while for the other filter the loss saturates after 13 epochs.
The latter presumably got stuck on a saddle point before and would decrease further with more training epochs.
This could have been an unfavorable weight initialization, but based on all shown cost evaluations, the loss with the $7 \times 7$ filter decreases much more and faster.
That means, there were either many saddle points very close to each other, or the performance of the $11 \times 11$ filter is actually worse.
Because more recent convolutional networks tend to use smaller filters, the latter theory is assumed.
Hence, a filter with a size of $7 \times 7$ is chosen for the first convolution.

The overall training losses for all networks are shown in \figref{fig:train-loss} and the test losses in \figref{fig:train-loss}.
One graph shows the moving averaged loss or accuracy, respectively, against the training iterations $\tau$.
For a direct comparison of the networks iterations are more suited than epochs, because due to the steadily increased dataset more iterations per epoch are executed. 
%This is capped at the smallest number of iterations of a network, in particular the 0-3 one, for an optimal comparison of training of all networks.
This is capped at the number of iterations of the 0-3 network for the single-category ones and at the number of iterations of the 4-3 network for the four-category ones for an optimal comparison of training of all networks.
For a full overview of the whole training process of each network the loss is plotted against the epochs.
For a more clear visualization of training and test, they are split into two separate graphs.

As expected, the 0-3 network starts with the smallest loss and keeps it throughout its training compared to all other networks, due to its simplicity.
Most closely to this comes the 0-4 network, however, with more rapid changes that are not visible for the moving averaged values.
Its loss is approximately from the 210-th iteration only slightly worse than the one of the 0-3 network.
This is expected as well because it is just slightly more complicated.
Moreover, it shows that the additional color class is not that challenging to classify.
The 0-5 and 0-6 network have the highest losses of all single-category networks.
This is not surprising, because they are challenged with the double color marks classification.
However, as training proceeds, their cost function is noticeable going to be minimized, it just takes longer due to their complexity compared to the other single-category networks.
It can be seen in \figref{fig:train-loss}, that the losses of 0-4, 0-5 and 0-6 are similar after 20 epochs, showing that the more complex networks just need a longer training, i.\,e. more iterations, for similar results.

The progress of the 4-0 network's loss behaves similar to the one of 0-3.
Furthermore, both losses match almost exactly after 280 iterations, i.\,e. at the end of 0-3's training.
However, the 4-0 loss gets steadily minimized further.
The remaining four-category networks behave as expected and similar to the single-category ones.
The more classes, in particular color marks, are used for classification, the higher the starting loss is.
That the loss of 4-5 and 4-6 is decreased faster at the beginning than the one of 4-3 and 4-4 is presumably due to a favorable weight initializing.
However, at approximately the 450-th iteration the loss of 4-3 and 4-4 is smaller than the one of 4-5 and 4-6, which is valid until the end of training.
This trend shows, that the less complex networks are faster minimized than the more complex ones.
Though, after 20 epochs, all losses of the four-category networks are similar minimized representing the behavior of the single-category networks.
Moreover, it can be seen that the 4-3 network presumably hit a saddle point in the range of approximately 150 to 250 iterations because its loss is consistent and then drops rapidly.
For the sake of completeness, the related accuracies of the training processes are shown in \figref{fig:networks-accuracy}.
Here the same effects can be seen as with the losses.
\begin{figure}
	\setlength\figureheight{.45\textwidth}
	\setlength\figurewidth{.9\textwidth}
	\centering
	\begin{subfigure}{\textwidth}
		\centering
		\input{images/train_loss_iteration.tikz}
		\caption{Moving averaged training losses of all networks capped at the number of iterations of the 0-3 network}
		\label{fig:train-loss-iteration}
	\end{subfigure}
	\begin{subfigure}{\textwidth}
		\centering
		\input{images/train_loss.tikz}
		\caption{Training losses of all networks}
		\label{fig:train-loss}
	\end{subfigure}
	\begin{subfigure}{\textwidth}
		\centering
		\input{images/test_loss.tikz}
		\caption{Test losses of all networks}
		\label{fig:test-loss}
	\end{subfigure}
	\caption{Training and test losses of networks}
	\label{fig:networks-loss}
\end{figure}
\begin{figure}
	\setlength\figureheight{.45\textwidth}
	\setlength\figurewidth{.9\textwidth}
	\centering
	\begin{subfigure}{\textwidth}
		\centering
		\input{images/train_accuracy_iteration.tikz}
		\caption{Moving averaged training accuracies of all networks capped at the number of iterations of the 0-3 network}
		\label{fig:train-accuracy-iteration}
	\end{subfigure}
	\begin{subfigure}{\textwidth}
		\centering
		\input{images/train_accuracy.tikz}
		\caption{Training accuracies of all networks}
		\label{fig:train-accuracy}
	\end{subfigure}
	\begin{subfigure}{\textwidth}
		\centering
		\input{images/test_accuracy.tikz}
		\caption{Test accuracies of all networks}
		\label{fig:test-accuracy}
	\end{subfigure}
	\caption{Training and test accuracies of networks}
	\label{fig:networks-accuracy}
\end{figure}
In either the losses or accuracies no indicator of overfitting is present, hence, all networks could be trained longer for achieving better results.
This is not performed in this work, though, due to a lack of time.
The minimum losses and maximum accuracies during training and testing of all networks are noted in \tabref{tab:network-performances} for reasons of comparability.
\begin{table}
	\centering
	\caption{Performance of all networks}
	\label{tab:network-performances}
	\begin{tabular}{c|c|c|c|c|c|c|c|c|c}
		& 0-3 & 0-4 & 0-5 & 0-6 & 4-0 & 4-3 & 4-4 & 4-5 & 4-6 \\ \hline
		Train Loss & 0.085 & 0.099 & 0.275 & 0.308 & 0.031 & 0.049 & 0.06 & 0.114 & 0.152 \\
		Test Loss & 0.027 & 0.065 & 0.226 & 0.386 & 0.146 & 0.26 & 0.258 & 0.339 & 0.5 \\ \hline
		Train Accuracy & 0.978 & 0.981 & 0.923 & 0.883 & 0.994 & 0.984 & 0.98 & 0.955 & 0.941 \\
		Test Accuracy & 1.0 & 0.991 & 0.948 & 0.84 & 0.972 & 0.926 & 0.935 & 0.889 & 0.856 \\
	\end{tabular}
\end{table}