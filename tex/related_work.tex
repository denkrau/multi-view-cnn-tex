\chapter{Related Work}
\label{sec:related-work}
Earlier 3D shape descriptors were mostly handcrafted based on a particular geometric property of the actual shape's surface or volume.
Those descriptors can be divided into two groups.
On the one hand, there are model-based 3D descriptors, that are directly based on the available 3D representation of objects that have been modeled using polygons, voxels, or point clouds, among others.
\textit{Osada et al.} \cite{Osada:2001:MMS:882486.884103} describe the signature of 3D objects by their shape distribution sampled from geometric shape functions.
Those functions use angles, distances, areas and volumes of an object's polygon mesh for forming a shape distribution.
By comparing the shape distribution of an object with the ones from similar and dissimilar objects, the class of the current object is found.
On the other hand, there are view-based 3D shape descriptors.
Those are created using multiple 2D views of an object instead of the raw 3D representation.
\textit{Chen et al.} \cite{Chen2003} introduced LightField Descriptors, the first typical view-based descriptor.
Such a descriptor is created by capturing images from 20 cameras positioned at the vertices of a dodecahedron with the solid 3D object inside.
By assuming that the views remain in this spatial order, the descriptor of another object can be compared by rotating it, until both sets are aligned.
There are 60 different variants of aligning them.
For each variant the distance of both sets is calculated by comparing each view with its aligned one and adding the result up.
The comparison is based on the light field representing the radiance in each view.
The minimum distance of all variants is used for the classification.
\textit{Shu et al.} \cite{Shu:2016:MCV:2965260.2965467} presented the Principal Thickness Images Descriptor that describes contour and volumetric information of an object.
First, they voxelize a 3D mesh object and perform a principle component analysis for yielding the three principle directions of the object.
Then, they measure the thickness along those axes by counting the number of voxels along each.
The thickness is finally encoded into a gray-scale image for each axis where the intensity of a pixel represents the number of voxels along the related axis.
With the histogram of oriented gradients a feature descriptor for each image is extracted.
In general, the advantages of view-based shape descriptors are their low dimensionality compared to model-based ones and, hence, the efficiency for processing.
In general, view-based descriptors have desirable properties like low dimensionality and efficient evaluation.
Moreover, they are more robust to noisy or holey 3D representations.

With the introduction of convolutional neural networks like AlexNet \cite{Krizhevsky:2012:ICD:2999134.2999257} and their improvement in image classification, they could be used for creating descriptors.
This performance was further improved with famous architectures like VGG-Net \cite{Simonyan15}, ResNet \cite{He2016ResNet} and Inception-v4 \cite{SzegedyInceptionv4}.
According to the ImageNet challenge \cite{Russakovsky:2015:ILS:2846547.2846559} results, the last two architectures outperform humans regarding the top-5 classification error in 2D object classification.
Using a variation of the VGG architecture, the so-called VGG-M \cite{journals/corr/ChatfieldSVZ14}, \textit{Su et al.} \cite{Su:2015:MCN:2919332.2919750} make their eight-layer multi-view convolutional neural network, short MVCNN, learn a compact shape representation of the actual object by collecting information from any number of input views without a specific order.
Previous methods combine the information of views with simple strategies like pairwise comparisons of descriptors or concatenating descriptors from ordered, consistent views.
The MVCNN architecture, however, performs a maximum pooling operation across all views for collecting discriminative features in a single shape descriptor.
This leads to an accuracy of 89.9\,\%, which outperforms state-of-the-art descriptors, for a network trained on the ImageNet1k dataset \cite{Krizhevsky:2012:ICD:2999134.2999257} and fine-tuned with the ModelNet40 one \cite{conf/cvpr/WuSKYZTX15} with 12 views per object.
That means, the network is first trained with single views from ImageNet1k and further trained with multi-views from ModelNet40.
In contrast, learning a single-view classification with an identical training and fine-tuning yields an accuracy of 88.6\,\%.
In this case, the accuracies of the corresponding 12 views are averaged before the overall accuracy is calculated.
Moreover, MVCNN outperforms 3D ShapeNets \cite{conf/cvpr/WuSKYZTX15} that reaches an accuracy of 77.3\,\% and is using a convolutional network on raw CAD data.
Hence, it is trained with the ModelNet40 dataset.
In the comparison of 2D- and 3D-representations from \textit{Su et al.} \cite{Su2018} the MVCNN architecture outperforms architectures working on point clouds and voxels.
Furthermore, they could improve the performance of the vanilla MVCNN to 95.0\,\% per instance by using a deeper network and better object centering.
However, \textit{Hegde et al.} introduce their FusionNet \cite{Hegde2016FusionNet3O} that combines a multi-view architecture, in particular \cite{Su:2015:MCN:2919332.2919750}, with a volumetric convolutional neural network for achieving each representation's advantages.
The 2D-representation is used for local spatial correlations, while the 3D-representation is used for long-range spatial correlations.
Their architecture achieves an accuracy of 93.11\% on the ModelNet10 dataset with 20 views and 90.80\% on ModelNet40 with 60 views.
According to \textit{Feng et al.} \cite{Feng2018}, view-to-shape descriptor methods like the one from \textit{Su et al.} are a milestone for 3D shape recognition and reflect the state-of-the-art.
Since in \cite{Su:2015:MCN:2919332.2919750} all views are weighted equally, their goal is to exploit the discriminability among views and their intrinsic hierarchical correlation.
Hence, they add a module that divides views with similar features into the same group.
Views inside a group are mean pooled for creating a group descriptor for each group.
Furthermore, a group with more discriminative views is associated with a higher weight than groups with less discriminative views.
Finally, a single weighted group descriptor is computed representing the shape descriptor of the object.
Because the grouping mechanism sounds promising for helping evaluate the information content of views, in particular, the one of views of the same object but with different material color marks, this work is based on it.
Their network yields an accuracy of 92.6\,\% with an identical training and configuration as before and the GoogLeNet or Inception-v1 architecture \cite{szegedy2015}, respectively.
Using only 8 views results in an accuracy of 93.1\,\%.
With transferring the MVCNN concept to the GoogLeNet architecture, an accuracy of 92.2\,\% instead of 89.9\,\% is achieved.
Another view grouping approach is presented by \textit{Cyr et al.} \cite{Cyr2004} using handcrafted descriptors.
They define similarity metrics based on curve matching for performing the view grouping.
Because views are redundant in a large part they can be reduced to a minimal set.
They introduce the aspects graph representation.
The theory behind is, that a small change in the vantage point of an object results in only a small change in the view projection.
However, for some views that change is large.
Those views represent a transition, the views between an aspect.
Hence, it is supposed, that the first describes the object satisfiable.