\subsection{Grouping Module: Generating Group Descriptors}
\label{sec:architecture-grouping-module}
The objective of the grouping module is grouping each view descriptor $\vec{V}^{[5]}_v$ of an object depending on its information content.
This weights discriminative ones more than others, hence, improves the classification performance.
Moreover, based on each view's discrimination score an analysis of its information content is supported.
The view descriptors $\vec{V}^{[5]}_v$ of each group $\mathbb{G}_g$ are then combined to a group descriptor $\vec{G}_g$.
Hence, this module's inputs are the view descriptors coming from the feature module.
\begin{figure}
	\centering
	\includegraphics[]{images/grouping_module_groups.pdf}
	\caption{Group creation and view sorting in grouping module. Each view descriptor $\vec{V}^{[5]}_v$ is propagated through the same fully-connected layer yielding a view discrimination score $\theta_v$ based on which the view or view descriptor, respectively, is divided into a group $\mathbb{G}_g$.}
	\label{fig:grouping-module-groups}
\end{figure}
First, the informational content of each view needs to be calculated.
The simplest and most intuitive way is to give every view a single number representing its score of discrimination.
One approach would be to make the score directly depend on the pixel values.
This could be performed with a fully-connected layer with the pixels as inputs and the score as output.
However, like with fully-connected neural networks, this leads to many additional parameters for the network due to the translation-variance of input values. 
This could be overcome by using convolutions first for extracting features for the score.
However, discriminative features are already extracted, presumably much more accurate than a few convolutions for the score would do. 
Hence, the followed approach is that each view's score depends on its most recent descriptor $\vec{V}^{[5]}_v$.
It is worth noting, that in \cite{Feng2018} the view scores do not depend on the last convolution.
However, their network architecture uses more than five convolutional layers, but the scores depend on the fifth one as well.
This is probably because later features of deeper view descriptors are too detailed and would result in too divergent discrimination scores for a reliable group generation.
In this work's architecture each latest view descriptor $\vec{V}^{[5]}_v$ is fed into the same single fully-connected layer with one node.
Therefore, the view descriptor matrix is flattened into a row-wise vector, which is multiplied with the corresponding weight matrix $\vec{W}^{[d4]}$ of shape $6 \cdot 6 \cdot 256 \times 1$.
To this result a bias $b^{[d4]}$ is added yielding the weighted sum $z^{[d4]}$ with a single value.
This is fed into its activation function.
In contrast to every other layer in this network, a leaky ReLU activation function is applied to this unit.
A dropout is not performed, because with only one node it is not desirable.
Dropping out this node represents a view discrimination score of 0.
This would generalize the view scores way to strong, hence, unnecessarily extending training time.
Finding well-suited parameters for this layer would presumable take longer than for the others, thus, generating a bottle-neck in the architecture.
%It was found during training, that the unit died at the beginning of the training most of the time when using ReLU activation functions.
%This is due to its characteristic.
%If the weighted sum is zero or below at the beginning of the training due to an unsuited weight initialization, the activation is zero, hence, the neuron dies immediately.
%Another reason could be a too large learning rate, allowing the weights to update in too big steps leading to a weighted sum of 0 or below.
%This results in a view score of $0$ for every view that is not changed during later training steps due to a gradient of 0.
%The learning rate should suit the whole network and not only this layer, though.
%Their gradient is always unequal to zero, hence, it is solving the dying ReLU unit problem.

For interpretation purposes, the activation is squeezed into a range from 0 to 1 using the sigmoid function according to \cite{Feng2018} representing a probability of discrimination.
Because the sigmoid function saturates at values higher than around $\abs{5}$, but the activation of the fully-connected layer is assumed to be larger, the natural logarithm of the activation is computed first.
This shifts the saturation to values that are presumably not in the range of the activation.
For having a continuous function the absolute value of the activation is taken beforehand.
This yields the expression for a view's score
\begin{equation}
	\label{eq:view-discrimination-score}
	\theta_v = \sigmoid\left( \log \left( \abs{a^{[d4]}} + \varepsilon \right) \right)
\end{equation}
where $a^{[d4]}$ is the output or activation, respectively, of the corresponding fully-connected layer.
The small constant $\varepsilon=10^{-6}$ is added for numerical stability for avoiding $\log(0)$.
A plot of this function is shown in \figref{fig:view-discrimination-score}.
\begin{figure}
	\setlength\figureheight{.45\textwidth}
	\setlength\figurewidth{.8\textwidth}
	\centering
	\input{images/view_discrimination_score.tikz}
	\caption{View discrimination score function plot}
	\label{fig:view-discrimination-score}
\end{figure}
All view discrimination scores for a batch are stored in a tensor $\bar{\vec{\Theta}}$ with shape $Batch \times Views$.

Dependent on each view score, the related views or view descriptors, respectively, are divided into groups.
For maximum flexibility, the number of groups $n_g$ equals the number of views $n_v$.
Hence, the size of each group bin $r_{\text{bin}}$ is related to the number of views $n_v$ per object and the range of possible view scores $r_\theta$.
With the limit of \eqref{eq:view-discrimination-score}
\begin{equation}
	\lim_{a^{[d4]}\rightarrow \infty} \sigmoid\left( \log \left( \abs{a^{[d4]}} + \varepsilon \right) \right) = 1
\end{equation}
and only positive values from the ReLU, scores are in the range $r_\theta = 1 - 0 = 1$.
Dividing $r_\theta$ in equal sized parts yields
\begin{equation}
	r_{\text{bin}} = \frac{r_\theta}{n_v} = \frac{1}{12} \approx 0.083
\end{equation}
as each group bin's size related to the discrimination score values.
Hence, a group $\mathbb{G}_g$ contains view descriptors $\vec{V}_v^{[5]}$ of views with scores of $(g-1) \cdot r_{\text{bin}} \leq \theta_v < g \cdot r_{\text{bin}}$ where $g \in \{1,2, \dots, n_v\}$.
\figref{fig:grouping-module-groups} illustrates the basic view score calculation and grouping.
This example assumes a group size of $0.33$.
The group to which each view $\tilde{\vec{X}}^{(i)}_v$ belongs is calculated by
\begin{equation}
	g_v = \floor \left( \frac{\vec{\theta}_v}{r_{\text{bin}}} \right)
\end{equation}
yielding the group's index, where $\vec{\theta}_v$ is the view score of the $v$-th view of the corresponding multi-view sample.
Those are stored for every view resulting in the group index vector $\vec{g}_{\text{idx}} = \left( g_1, g_2, \cdots, g_{n_v}\right)^T$.
The subscripts correspond to the view indices, e.g. $g_2$ belongs to $\tilde{\vec{X}}^{(i)}_2$.
With this approach, it is possible for groups to remain empty.
Based on the group indices, related view descriptors are averaged across their first dimension.
In brief, every feature is averaged.
This is illustrated in \figref{fig:grouping-module-group-descriptors}.
\begin{figure}
	\centering
	\includegraphics[]{images/grouping_module_group_descriptors.pdf}
	\caption[Generating group descriptors]{Generating group descriptors $\vec{G}_g$ by calculating the average of view descriptors $\vec{V}^{[5]}_v$ in each group $\mathbb{G}_g$.}
	\label{fig:grouping-module-group-descriptors}
\end{figure}
Because all view descriptors in a group should have similar extracted features, they can be combined by averaging them without a significant loss in information.
If the maximum of all features were taken, though, the group would presumably contain all view descriptors where important features were extracted for creating a group descriptor containing as many features as possible.
However, this is not desirable for the use case.
This results in a group descriptor
\begin{equation}
	\vec{G}_g = \frac{\sum_{\vec{D} \in \mathbb{G}_g} \vec{D}}{|\mathbb{G}_g|}
\end{equation}
with the same size as a view descriptor, where $\vec{D}$ is a view descriptor $\vec{V}_v^{[5]}$ of group $\mathbb{G}_g$.
The addition and division are calculated element-wise.
However, the views of different objects are not necessarily divided into the same number of groups, thus, leading to a different number of group descriptors.
For a valid matrix representation when using batches group descriptors of zero need to be added that the number of all batch's group descriptors match.
However, in the following $n_g$ is used for the group dimension for simplicity.
Hence, the final shape of the tensor $\bar{\vec{G}}$ containing all group descriptors is $Batch \times n_g \times 6 \times 6 \times 256$.

Every group $\mathbb{G}_g$ gets a weight $w_g$ assigned representing its discrimination.
This depends on the scores of its contained views.
Analog to before, views of a group are found by checking the group index vector $\vec{g}_{\text{idx}}$.
This time the related view scores are summed up and divided by their number for calculating a mean.
In mathematical terms,
\begin{equation}
	\hat{w}_g = \frac{\sum_{\vec{D} \in \mathbb{G}_g} \theta_v(\vec{D})}{|\mathbb{G}_g|}
\end{equation}
calculates the weight of the $g$-th group, where $\theta_v(\vec{D})$ is the score of a view descriptor $\vec{V}_v^{[5]}$ in $\mathbb{G}_g$.
Furthermore, the weight of a group is normalized by
\begin{equation}
	w_g = \frac{\hat{w}_g}{\max(\theta(\mathbb{G}_g))}
\end{equation}
to a range of 0 and 1, where $\max(\theta(\mathbb{G}_g))$ yields the maximum score of the given group, for being able to compare it with the ones in \cite{Feng2018}.
The calculation of the group weights is illustrated in \figref{fig:grouping-module-weights} referring to the example in \figref{fig:grouping-module-groups}.
\begin{figure}
	\centering
	\includegraphics[]{images/grouping_module_weights.pdf}
	\caption{Calculation of group weights in grouping module}
	\label{fig:grouping-module-weights}
\end{figure}
All group weights of an object are stored in a vector $\vec{w}_{\text{groups}}$ of size $n_g$.
The corresponding tensor $\bar{\vec{W}}_{\text{groups}}$ has a size of $Batch \times n_g$.