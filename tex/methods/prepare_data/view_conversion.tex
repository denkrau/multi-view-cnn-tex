\subsection{Single-View to Multi-View Conversion}
\label{sec:prepare-data-view-conversion}
The objective of the network is to classify multi-views of objects.
This means each input represents an object with all its corresponding views.
However, the views created in \secref{sec:dataset-material-feature} exist in single view representation because they are rendered successively.
Thus, each model's single views need to be converted into a related multi-view representation.
For achieving this, all single views are collected in order of their creation for keeping their relation.
Each view's pixel values are read, normalized to a range between 0 and 1, flattened to a vector $\vec{x}^{(i)}$ and stored in the input matrix $\vec{X}_{\text{set}}$ of the corresponding set.
Simultaneously to the view gathering each one-hot encoded label $\vec{y}^{(i)}$ needs to be created as well and put into the label matrix $\vec{Y}_{set}$ of the corresponding set similar to the views.
Each label may contain an object class, the so-called category, and a color mark class, the so-called color.
Depending on the type of classification, these two classes possibly need to be combined.
The single-label classification case embraces the following configurations.
If categories and colors are classified, both classes are appended to each other.
This results in $n_y = n_c \cdot n_m$ classes for classification, where $n_c$ is the number of categories and $n_m$ the number of color marks.
If only categories or color marks are classified, the final label contains only the category class or the material class, respectively.
Logically, the number of classes for classification, is either $n_y = n_c$ or $n_y = n_m$.
In the multi-label classification case, both labels are used independently.
Hence, the number of classes for classification equals $n_y = n_c + n_m$.
An example of those configurations is shown in \tabref{tab:label-generation}.
\begin{table}[]
	\centering
	\caption[Label generation]{Label generation with example category classes "bathtub" and "sofa" and color mark classes "0" and "1" for different cases of classifications.}
	\label{tab:label-generation}
	\begin{tabular}{l|l|l}
		Classification      & Single-Label                                                                        & Multi-Label                                                    \\ \hline
		Category + Material & \begin{tabular}[c]{@{}l@{}}bathtub\_0\\ bathtub\_1\\ sofa\_0\\ sofa\_1\end{tabular} & \begin{tabular}[c]{@{}l@{}}bathtub\\ sofa\\ 0\\ 1\end{tabular} \\ \hline
		Category            & \begin{tabular}[c]{@{}l@{}}bathtub\\ sofa\end{tabular}                              & n/a                                                            \\ \hline
		Material            & \begin{tabular}[c]{@{}l@{}}0\\ 1\end{tabular}                                       & n/a                                                           
	\end{tabular}
\end{table}
When the number of classes is known, a one-hot encoding is performed on each label.
Dividing each input and output matrix into datasets yields
\begin{subequations}
	\begin{align}
		\label{eq:dataset-train}
		\mathbb{D}_{\text{train}} &= \left( \vec{X}_{\text{train}}, \vec{Y}_{\text{train}} \right) \\
		\label{eq:dataset-test}
		\mathbb{D}_{\text{test}} &= \left( \vec{X}_{\text{test}}, \vec{Y}_{\text{test}} \right)
	\end{align}
\end{subequations}
where each column of $\vec{X}$ and $\vec{Y}$ of the same set are building an input-output pair.

Those single view and label representations need to be converted to a multi-view representation yielding $\tilde{\vec{X}}_{\text{set}}$ and $\tilde{\vec{Y}}_{\text{set}}$, respectively.
Because sorted data was read in, it is known that each $n_v=12$ elements belong together and need to put together somehow.
Looking at common definitions yield, that images are a three-dimensional matrix with the shape definition $Height \times Width \times Channels$.
Furthermore, tensorflow mostly uses the shape definition $Batch \times Height \times Width \times Channels$ for tensors.
Hence, reshaping each view back to a matrix representation of its original size and assuming it as a batch element is a reasonable approach for generating a multi-view of an object.
If an actual batch dimension becomes necessary it is inserted as a new first dimension.
Because each $n_v$ labels are identical, it is sufficient to just keep every $n_v$-th label for the multi-view representation.
For a later lookup of the labels, they are saved to the disk as a text file.