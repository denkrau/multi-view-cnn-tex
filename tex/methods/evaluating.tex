\section{Evaluating the Architecture}
\label{sec:methods-evaluate}
For a comprehensive analysis of the network architecture of this work, especially the grouping mechanism and the information content of views, a lot of data has to be collected.
Fortunately, every tensor can be gathered and, if necessary, manipulated to be interpretable.
The most important tensor for training contains the cost of every iteration of the training set because this is attempted to be minimized, because it represents how well the network classifies the data.
Each one is stored during training to be able to plot them afterwards.
Furthermore, after each epoch, the cost and accuracy of the whole training and test set are calculated with current parameters.
Because the ModelNet10 dataset is split by default only into a training and test set, the latter is used additionally as a validation set.
The cost and accuracy of each set is computed by computing them for each batch and averaging the results.
However, the last batch has in general fewer elements than the ones before.
Hence, a weighted average is performed with the batch sizes as the weights.
This yields the averaged cost
\begin{equation}
	J\left(\hat{\bar{\tilde{\vec{Y}}}}, \bar{\tilde{\vec{Y}}}\right) =  \frac{\sum_{i}^{n_b} m_{b_i} \cdot J\left(\hat{\bar{\tilde{\vec{Y}}}}^{(i)}, \bar{\tilde{\vec{Y}}}^{(i)}\right)}{\left|\vec{m_b}\right|}
\end{equation}
and the averaged prediction accuracy
\begin{equation}
	\alpha_\text{p}\left(\hat{\bar{\tilde{\vec{Y}}}}, \bar{\tilde{\vec{Y}}}\right) =  \frac{\sum_{i}^{n_b} m_{b_i} \cdot \alpha_\text{p}\left(\hat{\bar{\tilde{\vec{Y}}}}^{(i)}, \bar{\tilde{\vec{Y}}}^{(i)}\right)}{\left|\vec{m_b}\right|}
\end{equation}
where $\vec{m_b}$ is a vector containing the batch sizes.
As mentioned, this is performed separately on the training set and the test set.
Those results are also stored for plotting purposes.
Comparing both related units can reveal if training should be continued or if overfitting or underfitting occurs.
Because the cost value is more general and the accuracy rather for practical purposes, the first one is examined.
If the training loss decreases while the training loss decreases as well, the network improves and training should be continued.
However, if the training loss decreases while the test loss increases, overfitting is indicated.
The network does not generalize, but focuses on the features in the training set, hence, never seen data like the test set cannot be classified properly.
At the end of the training, the accuracy of the training set is defined as the performance of the network.
To plots, whose shown values $\vec{\omega}$ oscillate, a moving average $\vec{\Omega}$ of the values is added.
A single averaged value is calculated by
\begin{equation}
	\Omega_i = \frac{\sum_{j = \max(1,i-\eta+1)}^{i} \omega_j}{\max(0,i-\eta)}
\end{equation}
where $\eta$ is the window size, that defines how many values are taken into account for averaging a certain sample.
If the window is larger than the available number of samples, in particular in the beginning, the window size is adapted temporarily.
By default it is set to $\eta=\floor\left( 0.1 \left| \vec{\omega} \right| \right)$ for a dynamical size.

For evaluating the whole network model regarding its practical use, the accuracies for each class, each category, and each color are calculated separately.
That means for the second and third, that each category accuracy includes all related multi-views independent of color and each color class accuracy includes all related multi-views independent of the category.
In general, the accuracy states how many samples are classified correctly.
However, the overall accuracy can be misleading, although each class has almost the same number of objects because it does not represent if certain classes are better classified than others.
Hence, the precision and recall score is computed for each class as well.
Furthermore, a simplified confusion matrix is calculated containing every sample's prediction.
It is plotted against the ground-truth classes, where predictions of identical classes are counted up.
The grouping module needs to be evaluated as well.
It is supposed that in a classification of only the color, the views with visible color manipulations belong to the highest weighted group.
Hence, it is sufficient to examine if the most discriminative group only contains views with visible manipulations.
Because the colors of the manipulated faces are known, each pixel of a view in the top group is checked if it matches such an RGB value.
If there is a match with at least one pixel of a view, the view is considered grouped correctly.
All correctly grouped views $TP$ and not correctly grouped views $FP$ are counted.
Based on them a percentage
\begin{equation}
	\label{eq:metric-group}
	\alpha_{\text{precision}}^{(i)} = \frac{{TP}^{(i)}}{{TP}^{(i)} + {FP}^{(i)}}
\end{equation}
is calculated representing the precision of the grouping module for a given multi-view input $i$.
This is repeated for all multi-views of the same color.
The final precision for that color results from averaging the precision of every multi-view of that color and is considered its group metric.
This is performed for every color to analyze if different colored face manipulations yield different results. 

For predicting the classes and gathering information of certain inputs, multi-views of objects need to be given.
These build a common input tensor $\bar{\tilde{\vec{X}}}^{(i)}$ representing the $i$-th batch.
If the number of samples, that are going to be predicted, exceeds the defined batch size, they need to be divided into an appropriate number of batches.
This tensor is now propagated through the network as usual.
%Meanwhile, the activations of the first convolutional layer are stored for visualizing the extracted features in each view.
%Therefore, each view's activations are split across the last dimension that represents the features.
%Each feature's values $\vec{F}_i$ are normalized by
%\begin{equation}
%	\label{eq:normalize}
%	\vec{F}_{i,norm} = \frac{\vec{F}_i - \min(\vec{F}_i)}{\max(\vec{F}_i) - \min(\vec{F}_i)} 
%\end{equation}
%to a range of 0 and 1 for making it visualizable.
%Finally, each feature is saved as an gray-scale image.
Meanwhile, each view discrimination score, group weight, and group index is stored for showing them with their associated view afterwards.
%Furthermore, a saliency map $\vec{S}_v^{(i)}$ is computed for each view $\tilde{\vec{X}}_v^{(i)}$ showing how much each pixel influences the raw output of the network $\vec{z}^{(i)}$.
%Here, the direct output of the eighth layer is taken, that means no softmax is applied.
%A single saliency map can be defined as the derivative of the output with respect to a single view yielding
%\begin{equation}
%	\label{eq:saliency-map}
%	\vec{S}_j^{(i)} = \frac{\partial \vec{z}^{(i)}}{\partial \tilde{\vec{X}}_v^{(i)}}
%\end{equation}
%as a general expression.
%For plotting each saliency map is normalized with \eqref{eq:normalize} and visualized in gray-scale.