\section{Training the Architecture}
\label{sec:methods-training}
This work's multi-view network architecture is trained by inputting the generated multi-view images $\tilde{\vec{X}}_{train}^{(i)}$ and comparing the prediction $\hat{\vec{y}}^{(i)}$ of the network with the corresponding label $\tilde{\vec{y}}_{train}^{(i)}$.
First, a batch size needs to be chosen to define how many samples will be fed to the network at once.
In a training using only single-views, a batch size of 128 could be achieved.
Thus, using $n_v = 12$ views the batch size is reduced to $m_b = \floor(128 / 12) = 10$.
However, due to a limited memory size of 8GB of the experimental setup's GPU and the additional parameters for the multi-view training only a batch size of 8 supports training reliably.
This yields $n_{b,train} = m_{train} / 8$ batches for the training set and $n_{b,test} = m_{test} / 8$ for the test set.
Nevertheless, dividing each set into 8 samples will be odd in general.
Thus, the last batch is filled with all the remaining samples.
After each epoch, the training set is shuffled randomly, so that batches do not contain the same samples as before.
For simplicity, a sample in the shuffled list is still referred to by its current index.

For calculating the cost and the derivatives of the parameters a softmax cross entropy is performed in the single-label classification case.
For efficiency, tensorflow applies a softmax internally, so the unscaled predictions need to be fed.
Then, the softmax measures the probability error between the prediction and ground-truth labels, while assuming mutually exclusive classes.
In the multi-label classification case, sigmoid cross entropy is applied.
Here the sigmoid is calculated internally and not mutually exclusive classes are assumed.
For updating the parameters with the goal of minimizing the cost function the Adam optimizer is employed due to its advantages.

The predictions $\hat{\bar{\tilde{\vec{Y}}}}^{(i)}$ of the network need to be compared to the ground-truth labels $\bar{\tilde{\vec{Y}}}^{(i)}$ of the $i$-th batch for checking the network's accuracy.
Due to the batch operations, the single dataset samples in a batch are referred to as batch samples.
How the comparison is performed, though, depends on the type of classification.
In the case of a single-label one, the index of the largest value in the batch element prediction $\hat{\tilde{\vec{y}}}^{(i)}$ is located.
The same operation is performed on the corresponding ground-truth label $\tilde{\vec{y}}^{(i)}$.
Each index represents a certain class, which is declared the predicted or actual one, respectively.
Now a binary comparison of both indices is performed, resulting in 0 if they are different and 1 if they are equal.
This is repeated for each batch element while storing all results in a tensor $\bar{\vec{E}}^{(i)}$.
Finally, the prediction accuracy $\alpha_{\text{p}}^{(i)}$ of the $i$-th batch is calculated by
\begin{equation}
	\label{eq:accuracy-mean}
	\alpha_{\text{p}}^{(i)} = \frac{\sum_{j} \bar{E}_j^{(i)}}{\left|\bar{\vec{E}}^{(i)}\right|}
\end{equation}
for the current batch $i$.
In the case of multi-label classification, a probability threshold needs to be defined when a predicted feature is actually considered predicted.
In this case, the threshold is $p_{\text{thres}}=0.5$, hence, the values of the prediction vector can be rounded.
Now an identical binary comparison as before can be applied to both label vectors resulting in a tensor $\bar{\vec{E}}^{(i)}$ as well.
The prediction accuracy is calculated with \eqref{eq:accuracy-mean}.

Furthermore, an initial learning rate is necessary.
Because finding it by trial-and-error would be time-consuming, the approach of the cyclical learning rate is used for finding an optimal learning rate.
The learning rate is initialized with $\gamma = 10^{-5}$.
After processing each batch it is exponentially increased according to
\begin{equation}
	\gamma(\tau+1) = \lambda \gamma(\tau)
\end{equation}
where $\lambda = 1.1$ is the scaling factor and $\tau$ the iteration.
Its value can be chosen arbitrarily but should be in range for achieving a desirable precision in learning rates.
For each learning rate, the related cost function evaluation is stored.
Training is stopped when the current cost value is four times the second to last one, i.\,e. when a drastic deterioration in cost happens, for having a noticeable oscillating characteristic at the end.
For evaluation, the cost values are plotted against the learning rates.
On the basis of this, the range of optimal learning rates can be read where a steep descent in cost values happen.
Optimally, this must be performed for each different network architecture.