\subsubsection{Optimal Learning Rate}
\label{sec:improving-performance-learning-rate}
The learning rate is the most important hyperparameter in a neural network.
If it is too small, learning converges exactly, though, really slow.
If it is too large, the minimum is steadily overshot and learning maybe diverges.

\textit{Smith} introduces the cyclical learning rate \cite{DBLP:journals/corr/Smith15a}.
For being cyclical, the learning rate needs a lower and upper bound.
This builds a range of optimal learning rates.
With these values inbetween, the evaluations of the cost function are effectively decreased over time.
Hence, for finding that optimal range, the learning rate is initialized with a very small value and slightly increased after each training step.
For each of the steps the cost function is evaluated.
\figref{fig:optimal-learning-rate-range} shows a typical plot of the cost function against the learning rate on a logarithmic $x$-scale.
\begin{figure}
	\setlength\figureheight{.45\textwidth}
	\setlength\figurewidth{.9\textwidth}
	\centering
	\input{images/optimal_learning_rate_loss.tikz}
	\caption[Optimal Range of Learning Rates]{Optimal Range of Learning Rates Highlighted in Blue. The learning rate is increased exponentially.}
	\label{fig:optimal-learning-rate-range}
\end{figure}
It can be seen, that if the learning rate is small, the cost function does not improve noticeably.
When the learning rate gets into its optimal range, the cost function suddenly becomes smaller.
This continues with a large gradient, until the learning rate exits its optimal range.
This passage is detected when the cost function starts to oscillate.
If the learning rate still gets increased, the cost function starts to diverge eventually.
\textit{Smith} suggests a triangular learning rate policy which first increases the learning rate from the lower bound to the upper bound and then decreases it back to the lower bound.
Each change happens linearly.
The actual learning rate depends on the time step.

Another approach is doing warm restarts of the learning rate after several iterations, hence, it is called Stochastic Gradient Descent with Warm Restarts\cite{DBLP:journals/corr/LoshchilovH16a}.
This is especially combined with the gradient descent algorithm.
The idea behind it is to swing out of a tight local minimum if the algorithm seems to get stuck there.
If well-suited parameters are found for a cost function, the latter can slightly change if the dataset changes.
Hence, the once well-suited parameters lead to a worse cost now.
Therefore, a minimum in a flatter region needs to be found where a slight change has no big impact, i.e. a solution that is more generalized across datasets.
\figref{fig:sgdr-cost} illustrates this scenario.
\begin{figure}
	\centering
	\includegraphics[]{images/sgdr-cost.pdf}
	\caption[Cost Function of Different Datasets]{Cost Function of Different Datasets. If the parameters represent a minimum of a dataset's cost function, the same parameters do not inevitably represent the minimum of a different dataset's cost function.}
	\label{fig:sgdr-cost}
\end{figure}
This algorithm uses a cosine annealing of the learning rate.
This means the learning rate is decreased in the form of half a cosine curve.
This is called a cycle.
After this, it is set to its initial value and the annealing is repeated.
This process is visualized in \figref{fig:sgdr-annealing-normal}.
\begin{figure}
	\setlength\figureheight{.3\textwidth}
	\setlength\figurewidth{.45\textwidth}
	\centering
	\begin{subfigure}{.49\textwidth}
		\centering
		\input{images/sgdr-annealing.tikz}
		\caption{Cosine Annealing}
		\label{fig:sgdr-annealing-normal}
	\end{subfigure}%
	\setlength\figurewidth{.45\textwidth}
	\begin{subfigure}{.49\textwidth}
		\centering
		\input{images/sgdr-annealing-extended.tikz}
		\caption{Extended Cosine Annealing}
		\label{fig:sgdr-annealing-extended}
	\end{subfigure}
	\caption{Annealing of Stochastic Gradient Descent with Warm Restarts}
	\label{fig:sgdr-annealing}
\end{figure}
So, first, the cost in decreased until the learning rate reset happens.
Then, it is possible to overshoot the local minimum that much, that a different one is targeted.
However, this minimum is flat enough that another learning rate restart does not change the targeted minimum.
Due to the objective of finding flat regions it is advisable to steadily extend each learning rate cycle like it is shown in \figref{fig:sgdr-annealing-extended}.
It is assumed, that the more iterations are done, the flatter the found region gets.
Thus, the longer its minimum is searched.