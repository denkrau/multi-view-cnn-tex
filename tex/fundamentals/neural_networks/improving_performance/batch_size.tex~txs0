\subsubsection{Batch Size}
\label{sec:improving-performance-batch-size}
Let's do a brief recap of the terminology.
The batch size defines how many samples are propagated through the network at once.
For each sample within the gradients of the cost function with respect to all parameters are calculated and finally averaged over all samples.
The updates resulting from a batch are called a iteration.
Processing all samples from the training set is called an epoch.

For finding the optimal batch size, first examine the two extremes.
On the one hand, the whole training set can form a batch.
This way the best direction to a minimum can be calculated.
In terms of number of iterations this methods is the best.
However, it is very expensive in terms of resources, because usually amount ofdata can not be held in the RAM or GPU 




in general smaller learning rates willrequire more epochs.
smaller batch sizes are better suited  to smaller leRNING RATES given the noise estimate of the error gradient