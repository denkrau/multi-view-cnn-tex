\subsubsection{Activation Functions}
\label{sec:improving-performance-activation-functions}
The activation functions of a neural network affect its performance and convergence as well.
Common activation functions are shown in \figref{fig:activation-functions}.
The most common ones are going to be examined.
The sigmoid function recently has fallen out of favor.
One disadvantage is its saturation which leads to a very small gradient.
If this small gradient is often multiplied because of several layers, the gradient gets very small.
This vanishing gradient leads to very small parameter updates.
Furthermore, a well-suited weight initialization is required.
If they are too large, related neurons become saturated and the network will barely learn.
Hence, in both cases, convergence takes a very long time.
Another disadvantage is, that outputs are not zero-centered.
That means, that, for example, if all data is positive, all related gradients point into the same, either positive or negative, direction.
This leads to an undesired zigzag pattern of the parameter updates for several samples.
However, the use of mini-batches smooths out this effect.
The tanh activation function has a zero-centered output, though, the saturation of the output remains.
The ReLU activation function accelerates the convergence process of stochastic gradient descent compared to sigmoid and tanh.
According to \textit{Krizhevsky et al.}, it is six times faster on the CIFAR10 dataset \cite{Krizhevsky2009LearningML} compared to tanh activation functions \cite{Krizhevsky:2012:ICD:2999134.2999257}.
This is due to its linear, non-saturation form.
Another advantage is its simple and light computation.
Furthermore, it makes the activations sparse from the perspective of a neural network.
This means not all neurons are active due to the ReLU being zero for input values below zero.
Hence, the overall network is lighter.
However, its property of the evaluation to zero is also a disadvantage.
This results in a gradient of zero as well and therefore in no related parameter updates.
If the weights are initialized badly or an unfavorable update is applied, for example, due to a too large learning rate, a ReLU unit can die, because its evaluation and the gradient are zero for all further computations.
In this case, a unit is very unlikely to recover.
A solution forms the leaky ReLU, which adds a slight slope of like $\lambda = 0.01$ to the horizontal line of 0, preventing the gradient to become zero.
Hence, the unit can recover.
However, the results of this approach are very inconsistent.

Taking all that information into account yields the recommendation of the ReLU activation function, if the weights and learning rate are chosen carefully
Additionally, the fraction of dead units should be monitored.
If the number is still concerning, leaky ReLU activations should be applied.
The tanh activation should be generally preferred over the sigmoid one.

\begin{figure}
	\setlength\figureheight{.3\textwidth}
	\setlength\figurewidth{.5\textwidth}
	\centering
	\begin{subfigure}{.5\textwidth}
		\centering
		\input{images/threshold-activation.tikz}
		\begin{equation*}
		\phi(x) =
		\begin{cases}
		1 & x \geq \text{Bias $b$} \\
		0 & x < \text{Bias $b$}
		\end{cases}
		\end{equation*}
		\caption{Threshold}
		\label{fig:threshold-activation}
	\end{subfigure}%
	\hfill
	\begin{subfigure}{.5\textwidth}
		\centering
		\input{images/heaviside-activation.tikz}
		\begin{equation*}
		\phi(x) =
		\begin{cases}
		1 & x \geq 0 \\
		0 & x < 0
		\end{cases}
		\end{equation*}
		\caption{Heaviside}
		\label{fig:heaviside-activation}
	\end{subfigure}
	
	\begin{subfigure}{.5\textwidth}
		\centering
		\input{images/relu-activation.tikz}
		\begin{equation*}
		\phi(x) =
		\begin{cases}
		x & x \geq 0 \\
		0 & x < 0
		\end{cases}
		\end{equation*}
		\caption{Rectified Linear Unit}
		\label{fig:relu-activation}
	\end{subfigure}%
	\hfill
	\begin{subfigure}{.5\textwidth}
		\centering
		\input{images/leakyrelu-activation.tikz}
		\begin{equation*}
		\phi(x) =
		\begin{cases}
		x & x \geq 0 \\
		x\cdot \text{Slope $\lambda$} & x < 0
		\end{cases}
		\end{equation*}
		\caption{Leaky Rectified Linear Unit}
		\label{fig:leakyrelu-activation}
	\end{subfigure}
	
	\begin{subfigure}{.5\textwidth}
		\centering
		\input{images/sigmoid-activation.tikz}
		\begin{equation*}
		\phi(x) = \frac{1}{1+\exp(-x)}
		\end{equation*}
		\caption{Sigmoid}
		\label{fig:sigmoid-activation}
	\end{subfigure}
	\caption[Common activation functions]{Plots and equations of common used activation functions. Where the Bias $b$ is the threshold value and $\lambda$ adds a small slope. Usually, the latter is very small like $\lambda=0.01$.}
	\label{fig:activation-functions}
\end{figure}