\subsubsection{Feed-Forward Pass}
\label{sec:training-forward-pass}
The actual training step, i.e. the finding of optimal weights and biases, starts with propagating samples through the network.
Therefore, a dataset $\mathbb{D}$ containing $m$ pairs of inputs and corresponding labels is needed.
Performing a one-hot encoding on the labels and assuming in general flattened input matrices yields
\begin{equation}
	\label{eq:dataset-one-hot}
	\mathbb{D} = \left( \vec{X} , \vec{Y} \right)
\end{equation}
where $\vec{X} \in \mathbb{R}^{n_x \times m}$ and $\vec{Y} \in \mathbb{R}^{n_y \times m}$ are representing each input and label as vectors, respectively, forming matrices.
Hereby, $n_x$ represents the size of each input image and $n_y$ the number of classes.
This set is then divided into a training, validation and test set.
Each of them contains $m_{\text{train}}$, $m_{\text{valid}}$ or $m_{\text{test}}$ samples, respectively, where $m = m_{\text{train}} + m_{\text{valid}} + m_{\text{test}}$.
Furthermore, there is a neural network with $L$ layers each containing an arbitrary number of neurons.
Expressing the activation of every perceptron with \eqref{eq:perceptron-sum} would get very confusing for a whole network.
Hence, a matrix notation is preferred.
First, for every $j$-th perceptron in the $l$-th layer its weights are summarized in a vector
\begin{equation}
	\label{eq:weights-vector}
	\vec{w}^{[l]}_j =
	\begin{pmatrix}
		w^{[l]}_{j,1} & w^{[l]}_{j,2} & \cdots & w^{[l]}_{j,n^{[l-1]}_h}
	\end{pmatrix}^T
\end{equation}
containing single weights, where the superscript in square brackets denotes the layer and the subscript denotes the edge of (target neuron, preceding neuron).
The number of hidden neurons in the $l$-th layer is represented by $n^{[l]}_h$.
These conventions are maintained for all parameters for the rest of this thesis.
The bias of the $j$-th neuron in the $l$-th layer is just a scalar denoted as $b^{[l]}_j$. 
Now, every weight vector and bias can be combined to a matrix and vector, respectively, for each layer.
This yields
\begin{subequations}
\label{eq:parameters}
	\begin{align}
		\vec{W}^{[l]} &=
		\begin{pmatrix}
			\vec{w}^{[l]}_1 & \vec{w}^{[l]}_2 & \cdots & \vec{w}^{[l]}_{n^{[l]}_h}
		\end{pmatrix}^T
		\label{eq:weights}
		\\
		\vec{b}^{[l]} &=
			\begin{pmatrix}
				b^{[l]}_1 & b^{[l]}_2 & \cdots & b^{[l]}_{n^{[l]}_h}
			\end{pmatrix}^T
		\label{eq:biases}
	\end{align}
\end{subequations}
where $\vec{W}^{[l]} \in \mathbb{R}^{n^{[l]}_h \times n^{[l-1]}_h}$ and $\vec{b}^{[l]} \in \mathbb{R}^{n^{[l]}_h}$.
Using these matrices and vectors data can easily be forwarded through the network by building up on \eqref{eq:perceptron-activation}.
The weighted sum of all neurons of the $l$-th layer is computed as
\begin{equation}
	\label{eq:weighted-sum}
	\vec{z}^{[l]} = \vec{W}^{[l]} \vec{a}^{[l-1]} + \vec{b}^{[l]}
\end{equation}
with the activations vector $\vec{a}^{[l-1]}$ of the $l-1$-th layer.
Putting this in an activation function yields
\begin{equation}
	\label{eq:activations}
	\vec{a}^{[l]} = \phi\left(\vec{z}^{[l]}\right)
\end{equation}
for the $l$-th layers activations.
Performing this for every layer results in 
\begin{equation}
	\label{eq:feedforward}
	\hat{\vec{y}}^{(i)} = f(\vec{x}^{(i)}, \vec{W}, \vec{B})
\end{equation}
as the network's prediction for the $i$-th data sample $\vec{x}^{(i)} \in \mathbb{R}^{n_x}$ where $\hat{\vec{y}}^{(i)} \in \mathbb{R}^{n_y}$ and $\vec{W}$ and $\vec{B}$ are concatenated matrices storing the parameters.
This superscript in round brackets is part of the used convention.
Feeding this result into a sigmoid function forces the values to be in a range between 0 and 1 representing class probabilities.
This prediction needs to be compared with the ground-truth label $\vec{y}^{(i)}$ for checking how good the network performs and, hence, how well the parameters fit.
Those vectors can look, for example, like
\begin{subequations}
	\begin{align}
		\vec{y}^{(i)} &= \begin{pmatrix} 0 & 1 & 0 & 0 & 0 \end{pmatrix}^T \\
		\hat{\vec{y}}^{(i)} &= \begin{pmatrix} 0.54 & 0.28 & 0.2 & 0.63 & 0.96 \end{pmatrix}^T
	\end{align}
\end{subequations}
where the first is the ground-truth label and the second the prediction.
It can be clearly seen, that the prediction is completely wrong.
The actual ground truth class has the second smallest probability in the prediction.
Hence, the parameters need to be changed.
In theory, an identical representation is desired.
Because finding optimal parameters is an optimization problem, a metric for the performance of the network is necessary.
This is served by a loss function that maps parameters to a loss value.
The most common loss function for comparing two probability distributions of mutually exclusive classes is the cross-entropy loss function.
It is defined as
\begin{equation}
	H(y^{(i)},\hat{y}^{(i)}) = -(y^{(i)} \log(\hat{y}^{(i)}) + (1-y^{(i)}) \log(1-\hat{y}^{(i)}))
\end{equation}
for a single output representing two classes and as
\begin{equation}
	\label{eq:cross-entropy-loss}
	H(\vec{y}^{(i)}, \hat{\vec{y}}^{(i)}) = - \sum_{j}^{n_y} y_j^{(i)} \log (\hat{y}_j^{(i)})
\end{equation}
for multi-class classification \cite{murphy2013machine}, where $n_y$ is the number of classes, $\vec{y}^{(i)}$ the ground truth label and $\hat{\vec{y}}^{(i)}$ the predicted probabilities of the $i$-th sample.
Due to the one-hot encoding of the labels, only the positive class $y_p^{(i)}$ is taken into account in the loss computation.
Hence, \eqref{eq:cross-entropy-loss} is reduced to
\begin{equation}
	\label{eq:cross-entropy-loss-compact}
	H(\vec{y}^{(i)}, \hat{\vec{y}}^{(i)}) = - y_p^{(i)} \log (\hat{y}_p^{(i)})
\end{equation}
where $y_p^{(i)}$ and $\hat{y}_p^{(i)}$ denote the probability of the positive class and its corresponding prediction, respectively.
A visualization of this expression is shown in \figref{fig:cross-entropy}.
\begin{figure}
	\setlength\figureheight{.5\textwidth}
	\setlength\figurewidth{.8\textwidth}
	\centering
	\input{images/cross-entropy-loss.tikz}
	\caption[Cross-entropy loss]{Cross-entropy loss. Large deviations in probability are strongly penalized.}
	\label{fig:cross-entropy}
\end{figure}
It can be seen, that large deviations in probability are strongly penalized.
In the range of small deviations, the slope of the graph is small which leads to little changes in loss if the probability difference changes only slightly.
Using each sample's loss value for computing an averaged loss yields
\begin{equation}
	\label{eq:cross-entropy-cost}
	J(\vec{x}, \vec{W}, \vec{B}, \vec{y}) = J(\hat{\vec{y}}, \vec{y}) = \frac{1}{m_{\text{train}}} \sum_{i=1}^{m_{\text{train}}} H(\vec{y}^{(i)}, \hat{\vec{y}}^{(i)})
\end{equation}
as the cost function for the training process.
Computing the cost of a different set is possible by using the corresponding samples.
This function depends on all weights and biases as regression parameters, hence, it is highly dimensional.
Minimizing it with respect to the weights and biases yields optimal parameters for a given training set.