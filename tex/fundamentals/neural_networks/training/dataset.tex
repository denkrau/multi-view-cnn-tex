\subsubsection{Dataset Generation}
\label{sec:dataset-generation}
The whole training process is based on the dataset from which the network learns the correlations of input and label.
Usually, a dataset consists of input-label pairs, where the input is the data that is fed into the network and the label is the ground-truth.
In the case of a classification task, the label represents the category.
However, there is no general rule for the amount of data.
It can be said, that more data is better for generalization, but too many samples can lead to overfit the network to the shown data.
The latter means, that the network is trained to long or to intensive on the shown data.
The consequence is, that it adjusts its weights and biases to classify this data perfectly, but cannot reliably classify general, unknown data of same objects anymore, because they slighty differ.
Basically, the amount of data depends on the objective of the network.
For classifying whether an image is black or white, only a few training samples would be needed.
Is the objective classifying objects within images, it depends on the number of possible objects and their complexity.
If the objects are simple geometric shapes, then not as many samples are needed as if the objects are common objects like type of animals or cars.
For the latter the number of samples would probably be approximately 1000 examples per class.
Luckily, there are several datasets available, that are already sorted and labeled, like the MNIST handwritten digits or the ImageNet dataset.
Available datasets are not limited to images, but can contain CAD models like the ModelNet dataset which is used for this network architecture.

Samples of a dataset are split into a training and testing set, and sometimes a validation set\cite{James2014}.
The first contains data, the network trains on.
From this data correlations of input and label are found.
After arbitrary training steps where parameter changes happened, the performance of the network is tested on the validation set.
This is data, the network is not trained on.
The objective is to check if overfitting occurs.
If the accuracy of the training set increases, the accuracy of the validation set has to increase as well.
This shows that the network still learns and gets better.
If the accuracy of the training set increases, but the accuracy of the validation set stays the same or decreases, it is an indicator for overfitting.
The testing set is data the network is not trained on as well.
It serves as a final performance check of the network to confirm its general accuracy.
If no validation set is available, the testing set can be used.
How the dataset is split depends again on the number and complexity of samples and the objective.

For processing the dataset, a one-hot encoding\cite{Harris2012} of the labels is recommended.
Usually, the labels are categorical data.
This means, they contain label or string values, respectively, instead of numeric values, that the networks needs.
For example, there is a fashion variable with the values "boot", "sweatshirt" and "pants".
The network would not know how to interpret these.
Thus, these values need to be converted to numeric values.
Furthermore, if these label values are outputs of the network, it should be easily possible to convert them back from numeric values.
Hence, they are converted to integers that represent a category.
Referring to the example, this results in the numeric values 0, 1 and 2 for the labels "boot", "sweatshirt" and "pants", respectively.
But numeric values have a natural ordered relationship between each other, that neural networks could exploit.
The index of "pants" is higher than the one of "boot", but neither of these categories is better or worse than the other.
Therefore, the indices are one-hot encoded as well.
This means removing the integer representation and inserting binary variables for simulating existing features.
Applying this to the example results in the feature label vector $\vec{f}_1 = (0, 1, 0)^T$ for the "sweatshirt" label.
This vector has a length of the number of different categories available, where every element is 0 except the one of the corresponding category which is 1.
\tabref{tab:one-hot-encoding} summarizes this approach.
\begin{table}[]
	\caption[One-Hot Encoding of Categorical Data]{One-hot encoding of categorical data. First, categorical label values are transformed to numeric values representing a category index. Then, this is replaced with binary variables to represent features, that removes the natural relationship of numeric values to each other. This vector has a length of the number of different categories, where every element is 0 except for the corresponding category which is 1.}
	\label{tab:one-hot-encoding}
	\centering
	\begin{tabular}{l|l|l}
		Categorical   & Integer & One-Hot                   \\ \hline
		"Boot"       & 0       & $\vec{f}_0 = (1, 0, 0)^T$ \\
		"Sweatshirt" & 1       & $\vec{f}_1 = (0,1, 0)^T$  \\
		"Pants"      & 2       & $\vec{f}_2 = (0, 0, 1)^T$
	\end{tabular}
\end{table}