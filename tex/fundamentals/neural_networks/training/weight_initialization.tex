\subsubsection{Weight Initialization}
\label{sec:training-weight-initialization}
Before the actual training starts, the parameters, the weights and biases, of the network need to be initialized.
If this is done right, i.e. the values are in a range that supports training, optimization will be achieved in lesser or least time.
In the other case, a converging to optimal values can be impossible.
Reasons for this are the exploding or vanishing of gradients during backpropagation\cite{Hochreiter1991}.
In the backward-pass, the gradients are computed for every layer and are passed from end to beginning using the chain rule.
For example, the derivative of the sigmoid function as it can be seen in \figref{fig:sigmoid-derivative} is in the range of $(0, 0.25]$.
If this is multiplied several times, the gradients at the beginning are way smaller than at the end.
If the weights are too small or too large, this effect is intensified.
This is partly true for other activation functions like the ReLU as well.
But here the gradients can become very large too if the weights are really large.
None of these scenarios is desirable, because the optimal weights are either not reached or skipped.
\begin{figure}
	\setlength\figureheight{.4\textwidth}
	\setlength\figurewidth{.7\textwidth}
	\centering
	\input{images/sigmoid-derivative.tikz}
	\caption{Sigmoid function and its derivative}
	\label{fig:sigmoid-derivative}
\end{figure}
This will become more clear in \secref{sec:training-gradient-descent} when the expressions of backpropagation are presented.

If the weights are initialized with 0, every neuron would compute the same output.
This leads to an identical gradient for each one and therefore identical parameter updates.
All in all, this would reduce the network to a linear one.
Hence, a common initialization approach is using a Gaussian distribution like $N(\mu, \sigma^2) = N(0, 0.01)$.
However, this way the variance of this distribution of each neuron's output grows with the number of its inputs.
Therefore, a normalization of the variance of each neuron's output to 1 is performed.
This is done by scaling its weights by the square root of its number of inputs.
This can be derived with its $n_{\text{in}}$ inputs $\vec{a}^{[l-1]}$ and weights $\vec{w}^{[l]}$ by
\begin{align*}
	\Var(z^{[l]}_{j}) &= \Var \left( \sum_{k}^{n_{\text{in}}} w^{[l]}_{jk} a^{[l-1]}_k \right) \\
	&= \sum_{k}^{n_{\text{in}}} \Var \left( w^{[l]}_{jk} a^{[l-1]}_k \right) \\
	&= \sum_{k}^{n_{\text{in}}} \left[ \E(w^{[l]}_{jk})^2 \right] \Var(a^{[l-1]}_k) + \E \left[ (a^{[l-1]}_k) \right]^2 \Var(w^{[l]}_{jk}) + \Var(a^{[l-1]}_k) \Var(w^{[l]}_{jk}) \\
	&= \sum_{k}^{n_{\text{in}}} \Var(a^{[l-1]}_k) \Var(w^{[l]}_{jk}) \\
	&= (n_{\text{in}} \Var(\vec{w}^{[l]}_j)) \Var(\vec{a}^{[l-1]}) \\
\end{align*}
where zero mean inputs and weights are assumed and an identical distribution of all $\vec{w}^{[l]}_{j}$ and $\vec{a}^{[l-1]}$.
Now, $z_j^{[l]}$ needs to have the same variance as all of its inputs $\vec{a}^{[l-1]}$, which yields $\Var(\vec{W}^{[l]}) = 1/n_{\text{in}}$ as every weights variance.
Hence,
\begin{equation}
	\vec{W}^{[l]} = \frac{N(0,1)}{\sqrt{n_{\text{in}}}}
\end{equation}
initializes the weights.
This is mostly universal, but must be used for $\tanh$ activation functions.
A similar analysis is done by \textit{Glorot and Bengio} \cite{Glorot10understandingthe} whose recommendation is
\begin{equation*}
	\Var(\vec{W}^{[l]}) = \frac{2}{n_{\text{in}} + n_{\text{out}}}
\end{equation*}
where $n_{\text{in}}$ and $n_{\text{out}}$ is the number of neurons in the incoming and outgoing layer, respectively.
Their motivation is, that by doing the earlier variance calculations for the backpropagated signal, it turns out that
\begin{equation}
	\Var(\vec{W}^{[l]}) = \frac{1}{n_{\text{out}}}
\end{equation}
is needed for keeping the variance of the input and output the same.
Because in general the constraint $n_{\text{in}} = n_{\text{out}}$ is not fulfilled, they make a compromise by taking the average.
Though, these initializations are not valid for, for example, ReLU units, due to their positive mean.
Fortunately, \textit{He et al.} \cite{DBLP:journals/corr/HeZR015} states the initialization
\begin{equation}
	\vec{W}^{[l]} = N(0,1) \cdot \sqrt{\frac{2}{n_{\text{in}}}}
\end{equation}
especially for ReLU neurons.