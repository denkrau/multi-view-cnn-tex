\subsubsection{Weight Initialization}
\label{sec:training-weight-initialization}
Before the actual training starts, the parameters, the weights and biases, of the network need to be initialized.
If this is done right, i.e. the values are in a range that supports training, optimization will be achieved in lesser or least time.
On the contrary, a converging to optimal values can be impossible.
Reasons for this are the exploding or vanishing of gradients during backpropagation\cite{Hochreiter1991}.
In the backward-pass the gradients are computed for every layer and are passed from end to beginning using the chain rule.
For example, the derivative of the sigmoid function as it can be seen in \figref{fig:sigmoid-derivative} is in the range of $(0, 0.25]$.
If this is multiplied several times, the gradients at the beginning are way smaller than at the end.
If the weights are too small or too large, this effect is intensified.
This is partly true for other activation functions like the ReLU as well.
But here the gradients can become very large too, if the weights are really large.
None of these scenarios is desirable, because the optimal weights are either not reached or skipped.
\begin{figure}
	\setlength\figureheight{.4\textwidth}
	\setlength\figurewidth{.7\textwidth}
	\centering
	\input{images/sigmoid-derivative.tikz}
	\caption{Sigmoid Function and its Derivative}
	\label{fig:sigmoid-derivative}
\end{figure}
This will become more clear in \secref{sec:training-stochastic-gradient} when the expressions of backpropagation are presented.

If the weights are initialized with 0, every neuron would compute the same output.
This leads to an identical gradient for each one and therefore identical parameter updates.
All in all, this would reduce the network to a linear one.
Hence, a common initialization approach is using a Gaussian distribution like $N(\mu, \sigma^2) = N(0, 0.01)$.
However, this way the variance of this distribution of each neuron's output grows with the number of its inputs.
Therefore, a normalization of the variance of each neuron's output to 1 is performed.
This is done by scaling its weights by the square root of its number of inputs.
This can be derived with the $n$ inputs $\vec{x}$ and weights $\vec{w}$ by
\begin{align*}
	Var(z) &= Var \left( \sum_{i}^{n} w_i x_i \right) \\
	&= \sum_{i}^{n} Var \left( w_i x_i \right) \\
	&= \sum_{i}^{n} \left[ E(w_i)^2 \right] Var(x_i) + E \left[ (x_i) \right]^2 Var(w_i) + Var(x_i) Var(w_i) \\
	&= \sum_{i}^{n} Var(x_i) Var(w_i) \\
	&= (n Var(w)) Var(x) \\
\end{align*}
where zero mean inputs and weights are assumed and an identically distribution of all $w_i$ and $x_i$.
Now, $z$ needs to have the same variance as all of its inputs $\vec{x}$, which yields $Var(w) = 1/n$ as every weights variance.
Hence,
\begin{equation}
	\vec{W} = \frac{N(0,1)}{\sqrt{n}}
\end{equation}
initializes the weights.
A similar analysis is done by \cite{Glorot10understandingthe} whose recommendation is
\begin{equation*}
	Var(\vec{w}) = \frac{2}{n_{in} + n_{out}}
\end{equation*}
where $n_{in}$ and $n_{out}$ is the number of neurons in the incoming and outgoing layer, respectively.
Though, this is, for example, not valid for ReLU units, due to their positive mean.
Fortunately, \cite{DBLP:journals/corr/HeZR015} states the initialization
\begin{equation}
	Var(\vec{W})) = N(0,1) * \sqrt{\frac{2}{n}}
\end{equation}
especially for ReLU neurons.
