\subsection{Train a Neural Network}
\label{sec:neural-networks-training}
So far optimal weights and biases were assumed in all examples, that exactly model a desired function $f(\vec{x, \vec{W}, \vec{B}}) = \vec{y}$ where $\vec{W}$ and $\vec{B}$ store all parameters of a network.
But in practical terms, they need to be found first for generating the desired output.
This starts by collecting or generating and preparing a dataset from which the network can find correlations by changing the weights and biases.
Then, those parameters are randomly initialized.
Furthermore, the data samples of the dataset are used as input and are feed-forwarded through the network yielding a classification $\hat{\vec{y}}$.
This classification is evaluated by a cost function.
That result is back-propagated through the network by computing its gradients for changing the weights and biases.
The forward pass and backward pass are repeated with different samples until an termination condition is satisfied.
\figref{fig:training} illustrates this process.
Each of these steps is covered in the following sections.
\begin{figure}
	\centering
	\includegraphics{images/training.pdf}
	\caption[Training process]{Training process. The parameters are initialized once at the beginning. The training process includes the forward pass, the cost function evaluation,the backward pass and the changing of parameters. This is repeated with different data samples.}
	\label{fig:training}
\end{figure}

\input{tex/fundamentals/neural_networks/training/dataset}
\input{tex/fundamentals/neural_networks/training/weight_initialization}
\input{tex/fundamentals/neural_networks/training/forward_pass}
\input{tex/fundamentals/neural_networks/training/gradient_descent}
\input{tex/fundamentals/neural_networks/training/adam}