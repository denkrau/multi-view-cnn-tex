\subsection{Convolutional Neural Networks}
\label{sec:neural-networks-convolutional-neural-networks}
Convolutional neural networks are suited for image processing tasks because they are invariant regarding the position of an object within an image.
Moreover, they need fewer parameters than multilayer perceptrons for finding features because the weights of local features can be shared and applied to different regions of an image \cite{LeCun1998cnn, Lecun98}.
Hence, they yield higher accuracies on generalized images in less training time.
The latter refers to the process of finding well-suited parameters.
Convolutional neural networks do not have an as strict separation in multiple layers as multilayer perceptrons do.
They rather have a pool of several layers which can be arbitrarily connected, repeated, and tuned with respect to their parameters to fulfill one's needs as illustrated in \figref{fig:cnn-layers}.
\begin{figure}
	\centering
	\includegraphics[]{images/cnn_layers.pdf}
	\caption[Layers of a convolutional neural network]{Layers of a convolutional neural network. Each combination of convolutional layer and pooling layer or fully-connected and dropout can be arbitrarily repeated. Moreover, pooling layers and dropout regularization layers are optional. The last fully-connected layer is not combined with a dropout layer.}
	\label{fig:cnn-layers}
\end{figure}
Commonly, convolutional layers are combined with pooling layers and fully-connected layers with dropout regularization layers.
However, the latter combinations are optional.
Each of them is explained in the following sections.
Combinations and repetitions of those layers with their own hyperparameters are called architecture.
Usually, convolutional and pooling layers manipulate the input and the resulting activations before fully-connected layers appear.
There are different proposed architectures with their weights and biases available.
The most common ones are AlexNet \cite{Krizhevsky:2012:ICD:2999134.2999257}, VGG \cite{Simonyan15}, GoogLeNet \cite{szegedy2015} and, ResNet \cite{He2016ResNet}.

\input{tex/fundamentals/neural_networks/cnn/convolution.tex}
\input{tex/fundamentals/neural_networks/cnn/pooling.tex}
\input{tex/fundamentals/neural_networks/cnn/fully_connected.tex}