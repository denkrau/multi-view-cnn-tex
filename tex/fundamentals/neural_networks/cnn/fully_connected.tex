\subsubsection{Fully-Connected Layer}
\label{sec:cnn-fully-connected}
The last part of a convolutional neural network mostly consists of at least one fully connected layer.
Such a layer is identical to a perceptron layer in \figref{fig:multilayer-perceptron} but uses the outputs or activations, respectively, of the previous convolutional or pooling layer.
The objective is to combine several features that were detected and use them as attributes for classifying the input.
Due to the weights, some attributes are more significant than others.
For example, if four legs and a long snout are found, there is a dog in the image and not a cat.
If the task is to distinguish only between cats and dogs, the snout feature is weighted more than the leg feature.
For preventing overfitting and improving generalization, a fully-connected layer can be combined with the dropout regularization technique \cite{Srivastava:2014:DSW:2627435.2670313}.
This drops out nodes randomly during training with a given probability, i.\,e. changing their incoming and outgoing weights temporarily to zero.
Hence, their weights are not adapted.
The interpretation of the activations of the last fully-connected layer in the architecture is simplified by applying an additional softmax function that squashes them into a range of 0 and 1, whereas the sum of all equals 1, to represent percentages of confidence or a probability distribution, respectively \cite{Bishop2006}.
The prediction $\hat{y}$ of a class $c$ manipulated with the softmax function can be written as
\begin{equation}
	\label{eq:softmax}
	\hat{y}_c = \frac{\exp(a^{[L]}_c)}{\sum_{j}^{n_y} \exp(a^{[L]}_j)}
\end{equation}
where $\vec{a}^{[L]}$ are the activations of the last layer.
Because the softmax manipulation outputs a probability distribution, it can only be performed when the classes are mutually exclusive, i.\,e. when only one class is correct.
This is called single-label classification.
Otherwise, for multi-label classification, a sigmoid function can be used, that squashes each output of the network into a range of 0 and 1.
The softmax or sigmoid manipulation corresponds to the output block in \figref{fig:cnn-layers}.
Combining the last fully-connected layer with a dropout layer is not desirable because this would remove some predictions of classes.